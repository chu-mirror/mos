@
\section{Process}

As I discussed above, codes can be executed by interpreter.
Follow traditional terms, I call codes being in the midst of execution is a process.
We have two kinds of process, one running at driver layer, one running at subsystem layer.
The process running at driver layer is easy, you can simply stop thinking about operating system,
imagine a bunch of assembly code running on a machine.
Each CPU fetches codes from memory, executes it, yields I/O operations, reads or writes memory concurrently.
You can see that each CPU usually performs actions in a rather not related way, they acts as different interpreters.
Official documents call such process {\it hart}, from {\it hardware thread}.
I tend to think on the mental model of a single interpreter having different environment references,
but you can build another mental model of 
different interpreters which are seperated to each other but from a common template.
A complete discussion of {\it hart} is beyond this document's scope, 
consult {\sl The RISC-V Instruction Set Manual Volume 1} for a detailed definition.

But when we turn to the processes of subsystem layer, you will be facing a completely blank area, 
you can not rely on your hardware's knowledge to imagine about it.
It's operating system's job to define what a interpreter of subsystem layer looks like,
yes, it's my job.

We are now at one of the most important and most creative stages among all courses of an operating system's building.
First of all, follow linux, I call this kind of process {\it task}.
In a broad view, a task can invoke {\it ports} of lower layer's drivers to do I/O operations,
or interact with other subsystems to do some work, just as I have talked in the first chapter.
But consider the situation of invoking a driver's port,
the process sink into driver layer, what should we call this process now?
A hart? or a task? It seems the process has not changed a lot, just jump from one point
to another, the same as any function's invoking in the same layer.
And it has not changed a lot in fact, but I call this process hart now.
The difference between a hart and a task is depended on which interpreter is in use,
concisely, what the process can take effect on, how the process can take effect.
For a hart, it can see the status of CPU's registers, and can operate on these registers if it has the
appropriate privilege.
For a task, it can not operate on its registers directly, it perfomes actions in a higher lever of abstraction,
the actions' objects consist of data structures, blocks of memory, and have no idea about CPU's status, which
may act a decisive role in its behaviour.
A typical situation is that a hart do some settings of the CPU's status, 
then it float up to be a task. The settings decided the task's behaviour,
including virtual memory, stack's position, privilege, ...etc.
Come back to the first chapter's {\it environment reference},
the two kinds of interpreter have different definitions of reference to an environment,
I will explain it later.

The relation between hart and task is not fixed, a multi-thread system can divide a hart to a lot of tasks,
each has a different setting compared to others, a single-thread system's hart equal to task but interpreted
in different layers. {\tt MOS} is a multi-thread system, I use the design from classic {\tt UNIX} system.

But before we begin to describe what task looks like in {\tt MOS},
let's solve a problem occurring in the means of {\it process}.

@
\subsection{Lock}
If two processes run concurrently, what will happen if they both try to write a object at the same time?
If the writing is not small enough to be an undivided action, there may be a situation when
one process are writing a part of that object, another also begin its writing.
The result is uncertain, depends on machine's machenism and process's behavior. 

The problem is particularly important when {\tt MOS} is supposed to run on a multi-core machine,
means there are several harts. So, a solution should be given in driver layer,
use support from hardware directly.
One of the ways to do that is {\it spinlock}.
I will not talk about it thoroughly, for the implementation is borrowed from others,
and the design are teached again and again in different textbooks.
{\sl Operating System Concepts, Chapter 6,7} may be a good reference.

@
\subsubsection{Basic machenism}
The whole implementation is divided into two parts,
this part defined the basic machenism provided by {\it spinlock}.
<<spinlock's repertoire>>=
<<repertoire of driver layer>>

@
The data structure is exported to outside.
<<spinlock's interface>>=
struct spinlock {
	uint64 locked;
};

@ %def spinlock

@
Acquire and release the lock.
<<spinlock's functions>>=
void
acquire(struct spinlock *lk)
{

	while (__sync_lock_test_and_set(&lk->locked, 1) != 0)
		;
	
	__sync_synchronize();
}

void
release(struct spinlock *lk)
{
	__sync_synchronize();
	__sync_lock_release(&lk->locked);
}

@ %def acquire release

<<spinlock's interface>>=
void acquire(struct spinlock *);
void release(struct spinlock *);

@
Initialize a spinlock.
<<spinlock's functions>>=
void
initlock(struct spinlock *lk)
{
	lk->locked = 0;
}

@ %def initlock

<<spinlock's interface>>=
void initlock(struct spinlock *);

@
Because when we use a spinlock to declare a block of code to a critical section,
we do not care at which layer the code is running,
so the interface should be shared by both layers.

<<common codes>>=
<<spinlock's interface>>

@
\subsubsection{Enhanced Version}

@
\subsection{Time Interrupt}
Then, we are going to partition a hart to several tasks in this section.
You can imagine that a hart is a long white rope, I coloring on it with different colors.
The sections of red can be connected logically to a red rope, that's it.
The question then come into sight is, according to what scheme I color the rope.

There are different solutions to this question in history, as far as I have seen,
two major strategies.
One is painting a color as long as colors want continuously, colors may have their own considerations
on which coloring is the best. Another is dividing the long white rope into a lot of pieces,
color a piece at once, follow a general rule, no color can last itself unlimitedly.
The previous appeared in early days of {\tt Mac}, but is not adopted nowadays,
the latter one is the mainstream.
And of course, I follow the latter one.

Now, the solution is obvious, I need a ruler to divide this rope,
and the ruler for this rope is a clock.
Here we ask the machine for help again, we need to set up timer interrupts.
But only let the clock run is meaningless,
some actions should be performed when clock delivered its advice.
I refer the actions with [[intvec]], name from interrupt vector,
[[intvec]] also handles other interrupts, I will give it a detailed description later.

@
Configure timer interrupts, a scratch area per CPU should be allocated, 
and the interval of coming interrupts are specified by macro [[CLINT_INTERVAL]].

The structure of a scratch area:

$0\ldots2$: space for [[timervec]] to save register\par
$3$: address of {\tt CLINT MTIMECMP} register\par
$4$: desired interval(in cycles) between timer interrupts

<<ask for timer interrupts>>=
{
#define CLINT_INTERVAL 1000000 /* about 1/10th second in qemu */

	static uint64 timer_scratch[NCPU][5];
	int id = r_mhartid(); /* configuration per CPU */

	*(uint64 *)CLINT_MTIMECMP(id) =
		*(uint64 *)CLINT_MTIME + CLINT_INTERVAL;

	uint64 *scratch = timer_scratch[id]; /* set up scratch field */
	scratch[3] = CLINT_MTIMECMP(id);
	scratch[4] = CLINT_INTERVAL;
	w_mscratch((uint64)scratch);

	w_mtvec((uint64)intvec); /* set up timer interrupt handler */
	w_mstatus(r_mstatus() | MSTATUS_MIE); /* enable global interrupt */
	w_mie(r_mie() | MIE_MTIE); /* only timer interrupt now */
#undef CLINT_INTERVAL
}

@
A lot of details here, but I'm not planing to explain here. As I said in Appendix,
it's your job to read {\tt RISC-V}'s specification.

@
\subsection{Task}
By now, we have a rope waiting to be colored, and a rule used to divide the rope,
it's time to select the color, a bad analogy, means to give a concise description of tasks.

Consider that we have a rope colored, look at one point on it,
or just abandon this analogy, it's a moment that one task is in running, others are waiting to run.
Then we have two state of task at a specific moment, running and waiting.
We need a data structure to record it.

<<task's data>>=
struct task_info {
	int state; /* 0: waiting; 1: running */
	<<status of a task>> /* wait for further use */
};

@ %def task_info

@
We also need a stack to run C~code, but wait, why not placing all informations of a task to
the same location?
I allocate one pagesize per task.

<<task's data>>=
__attribute__ ((align (16))) struct task {
	struct task_info info;
	char stack[PGSIZE - sizeof(struct task_info)];
} tasks[NTASK];

@ %def tasks

<<repertoire of subsystem layer>>=
#define NTASK 64 /* maximum number of tasks */

@ %def NTASK

@
Because I do not want to bother at the dynamic memory allocation,
just set a limitation to number of tasks.

Okay, well done, seems like we are completing this topic,
but let's return to the model of layers for now.
A task is running at subsystem layer, it communicates happily with files, memories,
and maybe other tasks. In some cases, maybe the filesystem, need to know the task's name,
a subsystem is supposed to communicate with a lot of tasks after all.
The task should be labeled, or named, at least from the viewpoint of outside.
A name, or {\it pid} in most case, should be attached to the entity of a task.
I just use the index of array [[tasks]] as the pid, and put it to register {\it tp}(thread pointer} to
let the task know who itself is.
It's interesting to compare {\it pid} with {\it hartid}(from register mhartid), 
they play a same role of reference to environment, but in different layers.
Here we answered the question of the difference of two layers' reference to environment.

<<repertoire of subsystem layer>>=
static inline uint64 getpid() {
	uint64 x;
	asm volatile("mv %0, tp":"=r"(x));
	return x;
}

@ %def getpid
