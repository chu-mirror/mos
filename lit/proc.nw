@
\section{Process}

As I discussed above, codes can be executed by interpreter.
Follow traditional terms, I call codes being in the midst of execution is a process.
We have two kinds of process, one running at driver layer, one running at subsystem layer.
The process running at driver layer is easy, you can simply stop thinking about operating system,
imagine a bunch of assembly code running on a machine.
Each CPU fetches codes from memory, executes it, yields I/O operations, reads or writes memory concurrently.
You can see that each CPU usually performs actions in a rather not related way, they acts as different interpreters.
Official documents call such process {\it hart}, from {\it hardware thread}.
I tend to think on the mental model of a single interpreter having different environment references,
but you can build another mental model of 
different interpreters which are seperated to each other but from a common template.
A complete discussion of {\it hart} is beyond this document's scope, 
consult {\sl The RISC-V Instruction Set Manual Volume 1} for a detailed definition.

But when we turn to the processes of subsystem layer, you will be facing a completely blank area, 
you can not rely on your hardware's knowledge to imagine about it.
It's operating system's job to define what a interpreter of subsystem layer looks like,
yes, it's my job.

We are now at one of the most important and most creative stages among all courses of an operating system's building.
First of all, follow linux, I call this kind of process {\it task}.
In a broad view, a task can invoke {\it ports} of lower layer's drivers to do I/O operations,
or interact with other subsystems to do some work, just as I have talked in the first chapter.
But consider the situation of invoking a driver's port,
the process sink into driver layer, what should we call this process now?
A hart? or a task? It seems the process has not changed a lot, just jump from one point
to another, the same as any function's invoking in the same layer.
And it has not changed a lot in fact, but I call this process hart now.
The difference between a hart and a task is depended on which interpreter is in use,
concisely, what the process can take effect on, how the process can take effect.
For a hart, it can see the status of CPU's registers, and can operate on these registers if it has the
appropriate privilege.
For a task, it can not operate on its registers directly, it perfomes actions in a higher lever of abstraction,
the actions' objects consist of data structures, blocks of memory, and have no idea about CPU's status, which
may act a decisive role in its behaviour.
A typical situation is that a hart do some settings of the CPU's status, 
then it float up to be a task. The settings decided the task's behaviour,
including virtual memory, stack's position, privilege, ...etc.
Come back to the first chapter's {\it environment reference},
the two kinds of interpreter have different definitions of reference to an environment,
I will explain it later.

The relation between hart and task is not fixed, a multi-thread system can divide a hart to a lot of tasks,
each has a different setting compared to others, a single-thread system's hart equal to task but interpreted
in different layers. {\tt MOS} is a multi-thread system, I use the design from classic {\tt UNIX} system.

But before we begin to describe what task looks like in {\tt MOS},
let's solve a problem occurring in the means of {\it process}.

@
\subsection{Lock}
If two processes run concurrently, what will happen if they both try to write a object at the same time?
If the writing is not small enough to be an undivided action, there may be a situation when
one process are writing a part of that object, another also begin its writing.
The result is uncertain, depends on machine's machenism and process's behavior. 

The problem is particularly important when {\tt MOS} is supposed to run on a multi-core machine,
means there are several harts. So, a solution should be given in driver layer,
use support from hardware directly.
One of the ways to do that is {\it spinlock}.
I will not talk about it thoroughly, for the implementation is borrowed from others,
and the design are teached again and again in different textbooks.
{\sl Operating System Concepts, Chapter 6,7} may be a good reference.

@
\subsubsection{Basic machenism}
The whole implementation is divided into two parts,
this part defined the basic machenism provided by {\it spinlock}.
<<spinlock's repertoire>>=
<<repertoire of driver layer>>

@
The data structure is exported to outside.
<<spinlock's interface>>=
struct spinlock {
	uint64 locked;
};

@ %def spinlock

@
Acquire and release the lock.
<<spinlock's functions>>=
void
acquire(struct spinlock *lk)
{

	while (__sync_lock_test_and_set(&lk->locked, 1) != 0)
		;
	
	__sync_synchronize();
}

void
release(struct spinlock *lk)
{
	__sync_synchronize();
	__sync_lock_release(&lk->locked);
}

@ %def acquire release

<<spinlock's interface>>=
void acquire(struct spinlock *);
void release(struct spinlock *);

@
Initialize a spinlock.
<<spinlock's functions>>=
void
initlock(struct spinlock *lk)
{
	lk->locked = 0;
}

@ %def initlock

<<spinlock's interface>>=
void initlock(struct spinlock *);

@
Because when we use a spinlock to declare a block of code to a critical section,
we do not care at which layer the code is running,
so the interface should be shared by both layers.

<<common codes>>=
<<spinlock's interface>>

@
\subsubsection{Enhanced Version}

@
\subsection{Time Interrupt}
Then, we are going to partition a hart to several tasks in this section.
You can imagine that a hart is a long white rope, I coloring on it with different colors.
The sections of red can be connected logically to a red rope, that's it.
The question then come into sight is, according to what scheme I color the rope.

There are different solutions to this question in history, as far as I have seen,
two major strategies.
One is painting a color as long as colors want continuously, colors may have their own considerations
on which coloring is the best. Another is dividing the long white rope into a lot of pieces,
color a piece at once, follow a general rule, no color can last itself unlimitedly.
The previous appeared in early days of {\tt Mac}, but is not adopted nowadays,
the latter one is the mainstream.
And of course, I follow the latter one.

Now, the solution is obvious, I need a ruler to divide this rope,
and the ruler for this rope is a clock.
Here we ask the machine for help again, we need to set up timer interrupts.
But only let the clock run is meaningless,
some actions should be performed when clock delivered its advice.
I refer the actions with [[intvec]], name from interrupt vector,
[[intvec]] also handles other interrupts, I will give it a detailed description later.

@
Configure timer interrupts, a scratch area per CPU should be allocated, 
and the interval of coming interrupts are specified by macro [[CLINT_INTERVAL]].

The structure of a scratch area:

$0\ldots2$: space for [[timervec]] to save register\par
$3$: address of {\tt CLINT MTIMECMP} register\par
$4$: desired interval(in cycles) between timer interrupts

<<ask for timer interrupts>>=
{
#define CLINT_INTERVAL 1000000 /* about 1/10th second in qemu */

	static uint64 timer_scratch[NCPU][5];
	int id = r_mhartid(); /* configuration per CPU */

	*(uint64 *)CLINT_MTIMECMP(id) =
		*(uint64 *)CLINT_MTIME + CLINT_INTERVAL;

	uint64 *scratch = timer_scratch[id]; /* set up scratch field */
	scratch[3] = CLINT_MTIMECMP(id);
	scratch[4] = CLINT_INTERVAL;
	w_mscratch((uint64)scratch);

	w_mtvec((uint64)intvec | (r_mtvec() & MTVEC_MODE_MASK)); /* set up timer interrupt handler */
	w_mstatus(r_mstatus() | MSTATUS_MIE); /* enable global interrupt */
	w_mie(r_mie() | MIE_MTIE); /* only timer interrupt now */
#undef CLINT_INTERVAL
}

@
A lot of details here, but I'm not planing to explain here. As I said in Appendix,
it's your job to read {\tt RISC-V}'s specification.

@
\subsection{Task}
By now, we have a rope waiting to be colored, and a rule used to divide the rope,
it's time to select the color, a bad analogy, means to give a concise description of tasks.

Consider that we have a rope colored, look at one point on it,
or just abandon this analogy, it's a moment that one task is in running, others are waiting to run.
Then we have two state of task at a specific moment, running and waiting.
We need a data structure to record it.

<<task's data>>=
struct task_info {
	<<status of a task>> /* wait for further use */
};

@ %def task_info

<<status of a task>>=
enum task_state state;

<<common codes>>=
enum task_state {<<states of a task>>};

<<states of a task>>=
RUN, WAIT

@ %def RUN WAIT

@
We also need a stack to store running state and for C~code's use,
but wait, why not placing all informations of a task to the same location?
I allocate one pagesize per task for this use.

<<task's data>>=
__attribute__ ((aligned (16))) struct task {
	struct task_info info;
	char stack[PGSIZE - sizeof(struct task_info)];
} tasks[NTASK];

@ %def tasks

<<repertoire of driver layer>>=
#define NTASK 64 /* maximum number of tasks */

@ %def NTASK

@
Because I do not want to bother at the dynamic memory allocation,
just set a limitation to number of tasks.

Okay, well done, seems like we are completing this topic,
but let's return to the model of layers for now.
A task is running at subsystem layer, it communicates happily with files, memories,
and maybe other tasks. In some cases, maybe the filesystem, need to know the task's name,
a subsystem is supposed to communicate with a lot of tasks after all.
The task should be labeled, or named, at least from the viewpoint of outside.
A name, or {\it pid} in most case, should be attached to the entity of a task.
I just use the index of array [[tasks]] as the pid, and put it to register {\it tp}(thread pointer} to
let the task know who itself is.
It's interesting to compare {\it pid} with {\it hartid}(from register mhartid), 
they play a same role of reference to environment, but in different layers.
Here let's answer the question of the difference of two layers' reference to environment.
Both types of process face a environment in which they execute,
and this enviroment can be represented by the {\it id} number in the context.
A {\it pid} is associated with the position of stack, or other entities belong to means of environment;
a {\it hartid} can tell where the hart is, this geographical information decide which interrupt will
visit this hart, and some other arguments, depend on hardware's designer.

<<repertoire of subsystem layer>>=
static inline uint64 getpid() {
	uint64 x;
	asm volatile("mv %0, tp":"=r"(x));
	return x;
}

@ %def getpid

@
\subsection{Switching}
In this section, we are going to handle interrupts, timer interrupt specifically,
that have been refered in previous section,
means we should confront the machine from now on.
I will give a detailed configuration of machine at first, then implement
first edition of [[intvec]].

If you are familiar with {\it RISC-V} architecture, you may wonder why I
introduce such a code organization, for a better seperation introduced by hardware
already exsited---{\it RISC-V} defined three modes: machine, supervisor, and user,
each has different previlege enforced by hardware.
An operating system built for {\it RISC-V} architecture is usually start from
supervisor mode, and use standardized interface, called {\it SBI}, to communicate with hardware.
But this project is for my self teaching, it should not concern too much about details of engineering,
and I do not like putting another layer of abstraction between machine and me, at least in this project.
So I decide to use machine layer directly, and do not use supervisor mode for simplicity.
But this decision means I can not use virtual memory in kernel space, but it's not a problem in such a small kernel,
and may reduce overheads of introducing virtual memory to kernel space.

@
Another thing worth to mention is, I use a monolithic interrupt vector to handle all interrupts.
To improve efficiency, set mode to {\it vectored}.
<<configure vectored trap vector>>=
w_mtvec((r_mtvec() & ~MTVEC_MODE_MASK) | MTVEC_VECTORED);

@
The main configuring has been done in [[<<ask for timer interrupts>>]],
remaining part will be introduced in following chapters.

@
Now, turn to the implementation of [[intvec]].
Because I set the mode to {\it vectored},
hart will jump to correct position according to value of {\it mcause}.
<<intvec's functions>>=
.globl intvec
.balign 256, 0
intvec:
	j sync_int
	nop
	nop
	nop
	nop
	nop
	nop
	j timer_int

timer_int:
	<<handle timer interrupt>>

sync_int:

<<repertoire of driver layer>>=
void intvec(void);

@
In handling a timer interrupt, two things need to be done:
set up next timer interrupt and swtich to next task.

<<handle timer interrupt>>=
<<set up next timer interrupt>>
<<switch to next task>>
mret

<<set up next timer interrupt>>=
csrrw a0, mscratch, a0
sd a1, 0(a0)	# save registers
sd a2, 8(a0)
sd a3, 16(a0)

ld a1, 24(a0)	# MTIMECMP register's location
ld a2, 32(a0)	# interval
ld a3, 0(a1)	# MTIMECMP register
add a3, a3, a2
sd a3, 0(a1)

ld a3, 16(a0)
ld a2, 8(a0)
ld a1, 0(a0)
csrrw a0, mscratch, a0

<<switch to next task>>=
# call switch
.globl test_timer
call test_timer

<<intvec's repertoire>>=
.globl switch

@
To let [[switch]] do the right things,
we complete {\it task} module.
{\it task} module run at driver layer. It's hart doing the settings of a task.

<<task's repertoire>>=
<<repertoire of driver layer>>

@
We need a way to create a new task, so, we should know how to get a unused field of [[tasks]].
It's a similar question to know what pids are in using, and if we solve this, we know another.
I use a list to save used pids, for convenience, I say the booting process is task 0.
Task 0 should never be running, use a new state to express it, [[SLEEP]].

<<status of a task>>=
int next;
int prev;

<<task's data>>=
struct spinlock task_list_lock;

<<task's macros>>=
#define next(pid) (tasks[(pid)].task_info.next)
#define prev(pid) (tasks[(pid)].task_info.prev)
#define state(pid) (tasks[(pid)].task_info.state)

<<states of a task>>=
,SLEEP

@ %def SLEEP

<<task's function>>=
void
inittask()
{
	ASM("addi tp, 0");

	state(pid) = SLEEP;

	next(pid) = 0;
	prev(pid) = 0;

	initlock(&task_list_lock);
}

@ %def inittask

<<repertoire of driver layer>>=
void inittask(void);

@
I use the strategy of finding the nearest blank field clockwise, means plus current pid.
Consider the newly created task's state, it can be [[WAIT]], but for further usage,
use a new state, [[NEW]].

<<states of a task>>=
,NEW

<<task's function>>=
int
newtask()
{
	int pid = getpid();
	while((pid+1 % NTASK) == next(pid)) {
		pid++;
	}
	next(pid) = pid + 1;

	pid++;
	next(pid) = next(pid-1);
	prev(pid) = pid - 1;

	prev(next(pid)) = pid;

	state(pid) = NEW;

	return pid;
}

@ %def newtask

<<common codes>>=
int newtask(void);

@
Switching to next task is a hard quest, we should store the old one's context,
and build context for new one. There are two possibilities of building context,
for a state of [[WAIT]], or for a state of [[NEW]].
Choosing which task as the next task is also a problem,
I simply choose the nearest task of [[WAIT]] or [[NEW]] from the list counterclockwise.

<<task's functions>>=
/*
void
switch()
{
	int curpid;
	int nextpid;

	curpid = getpid();
	do {
		nextpid = prev(curpid);
	} while (state(nextpid) == WAIT || state(nextpid) == NEW);

	state(curpid) = WAIT;
	change_context(curpid, nextpid);
}
*/

void
test_timer()
{
	printf("Hello from CPU%d\n", r_mhartid());
}

<<repertoire of driver layer>>=
void test_timer(void);

<<task's repertoire>>=
<<log's interface>>
