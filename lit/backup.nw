
@
Configure clock interrupts, a scratch area per CPU should be allocated, 
and the interval of coming interrupts are specified by macro [[CLINT_INTERVAL]].

The structure of a scratch area:

$0\ldots2$: space for [[timervec]] to save register\par
$3$: address of {\tt CLINT MTIMECMP} register\par
$4$: desired interval (in cycles) between timer interrupts

<<start's repertoire>>=
<<kernelvec's interface>>

<<ask for clock interrupts>>=
{
#define CLINT_INTERVAL 1000000 /* about 1/10th second in qemu */

	static uint64 timer_scratch[NCPU][5];
	int id = r_mhartid(); /* configuration per CPU */

	*(uint64 *)CLINT_MTIMECMP(id) =
		*(uint64 *)CLINT_MTIME + CLINT_INTERVAL;

	uint64 *scratch = timer_scratch[id];
	scratch[3] = CLINT_MTIMECMP(id);
	scratch[4] = CLINT_INTERVAL;
	w_mscratch((uint64)scratch);

	w_mtvec((uint64)timervec);
	w_mstatus(r_mstatus() | MSTATUS_MIE);
	w_mie(r_mie() | MIE_MTIE);
#undef CLINT_INTERVAL
}


@
\subsubsection{Subsystem Layer}
Some advanced features can be introduced here

@
Define the data structure first, here used a fixed amount of {\it locks},
it's relatively reasonable design when compare the need for a new lock to
overhead produced by dynamic memory allocation.

<<lock's data>>=
struct spinlock {
	uint64 locked;
} locks[MAX_LOCK];

/* current number of locks in use, [[locks[0]]] is used by itself */
static int cur = 1; 

<<lock's macros>>=
#define MAX_LOCK 64

<<lock's interface>>=
typedef struct spinlock *spinlock;


@
Initialize this module, the array of locks should it self be locked.
<<lock's functions>>=
void
spinlock_init()
{
	init_lock(&locks[0]);
}

void
initlock(spinlock lk)
{
	lk->locked = 0;
}

@
Lock's allocation and release.
<<lock's functions>>=
spinlock
allo_lock()
{
	if (cur == MAX_LOCK) {
	}
}
